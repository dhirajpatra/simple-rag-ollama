# ğŸ§  Simple RAG-based PDF Chatbot (FastAPI + Streamlit + Ollama)

This project is a simple Retrieval-Augmented Generation (RAG) chatbot using:

- **FastAPI** as the backend
- **Streamlit** as the frontend
- **Ollama** for embedding and chat models
- **User PDFs** stored in `documents/` folder for context

---

## ğŸ“‚ Folder Structure

```

.
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ rag\_engine.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ requirements.txt
â”œâ”€â”€ documents/                  # Place all PDFs here
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ entrypoint.sh              # Used in ollama service
â””â”€â”€ README.md

````

---

## ğŸ“¦ Models Used

- Embedding Model: `hf.co/CompendiumLabs/bge-base-en-v1.5-gguf`
- Language Model: `hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF`

These are automatically pulled by the `ollama` container via `entrypoint.sh`.

---

## ğŸš€ How to Run

### 1. Place your PDFs

Put all your PDF files into the `documents/` folder.

### 2. Start the Application

```bash
./start.sh
````

---

## ğŸŒ Usage

* Go to `http://localhost:8501`
* Ask questions based on your uploaded PDFs
* Backend uses RAG (embedding + cosine similarity + LLM) to generate answers

---

## ğŸ‹ Services

* **Ollama**: Loads models and performs embedding + chat
* **Backend (FastAPI)**: Handles query and performs retrieval + generation
* **Frontend (Streamlit)**: Simple UI to ask questions

---

## ğŸ“ License

MIT
